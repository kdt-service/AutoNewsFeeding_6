{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = datetime(2023, 2, 1)\n",
    "end_date = datetime(2023, 2, 2)\n",
    "\n",
    "# 크롤링할 URL 설정\n",
    "url = \"https://news.daum.net/breakingnews/culture\"\n",
    "\n",
    "# 크롤링할 기사 링크를 저장할 리스트 초기화\n",
    "links = []\n",
    "\n",
    "# csv파일에 저장할 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 주어진 기간 동안 각 페이지의 기사 링크 추출\n",
    "while start_date <= end_date:\n",
    "\n",
    "    # 현재 날짜에 해당하는 페이지 URL 생성\n",
    "    date_str = start_date.strftime(\"%Y%m%d\")\n",
    "    page_url = f\"{url}?regDate={date_str}\"\n",
    "    num_page = 1\n",
    "    \n",
    "    # 각 페이지의 기사 링크 추출\n",
    "    while True:\n",
    "        page_url_num = f\"{page_url}&num_page={num_page}\"\n",
    "        res = requests.get(page_url_num)\n",
    "\n",
    "        # HTTP 요청이 성공했는지 확인\n",
    "        if res.status_code != 200:\n",
    "            print(f\"Failed to retrieve page: {page_url_num}\")\n",
    "            continue\n",
    "\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        # 각 기사의 링크를 추출하여 리스트에 추가\n",
    "        article_links = soup.select(\".list_news2 a.link_txt\")\n",
    "        if not article_links:\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            article_url = link[\"href\"]\n",
    "            article_res = requests.get(article_url)\n",
    "            article_soup = BeautifulSoup(article_res.content, \"html.parser\")\n",
    "            date = article_soup.select_one(\".info_view .txt_info\").get_text().strip()\n",
    "            if(article_url, date) not in links:\n",
    "                links.append((article_url, date))\n",
    "\n",
    "        num_page += 1\n",
    "    \n",
    "    # 다음 페이지로 넘어가기 위해 날짜 1일 증가\n",
    "    start_date += timedelta(days=1)\n",
    "\n",
    "# 추출된 링크를 이용하여 기사 내용 크롤링\n",
    "    for link, date in links:\n",
    "        # 기사 URL 출력 (테스트용)\n",
    "        print(f\"\\n☞링크: {link}\")\n",
    "    \n",
    "        # 기사 HTML 코드 가져오기\n",
    "        res = requests.get(link)\n",
    "\n",
    "        # HTTP 요청이 성공했는지 확인\n",
    "        if res.status_code != 200:\n",
    "            print(f\"Failed to retrieve article: {link}\")\n",
    "            continue\n",
    "\n",
    "            # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "            # 기사 제목 추출\n",
    "        title = soup.select_one(\".tit_view\").get_text()\n",
    "\n",
    "            # 기사 본문 추출\n",
    "        contents = soup.select(\".article_view p\")\n",
    "        content = \"\\n\".join([c.get_text().strip() for c in contents])\n",
    "\n",
    "            # 추출한 제목과 내용 출력\n",
    "    \n",
    "        print(f\"☞기사제목: {title}\")\n",
    "        print(f\"☞작성일자: {date_str}\")\n",
    "        print(f\"☞기사내용: \\n{content}\")\n",
    "\n",
    "        rows.append([date_str, title, content])\n",
    "        \n",
    "# csv 파일로 저장\n",
    "    with open(\"news_Culture.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            # 마이크로소프트 엑셀은 UTF-8 인코딩을 인식하지 못할 수 있다.  \n",
    "            # 엑셀에서 인식할 수 있는 인코딩인 \"cp949\" (한국어 Windows 기본 인코딩)를 넣으면 정상적으로 열릴 것이다.\n",
    "            # 만약, 엑셀이 아닌 다른 프로그램에서 파일을 열때 cp949인코딩이 문제될 수 있으므로 utf-8을 사용할 것.\n",
    "#    with open(\"news_Culture.csv\", \"w\", newline=\"\", encoding=\"cp949\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Date\", \"Title\", \"Content\"])\n",
    "        writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
