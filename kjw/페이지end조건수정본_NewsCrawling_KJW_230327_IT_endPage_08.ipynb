{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63cf144",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to retrieve article: \u001b[39m\u001b[39m{\u001b[39;00marticle_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m article_soup \u001b[39m=\u001b[39m BeautifulSoup(article_res\u001b[39m.\u001b[39;49mcontent, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     52\u001b[0m date \u001b[39m=\u001b[39m article_soup\u001b[39m.\u001b[39mselect_one(\u001b[39m\"\u001b[39m\u001b[39m.info_view .txt_info\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\site-packages\\bs4\\__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    334\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\site-packages\\bs4\\__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    452\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    400\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    401\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m startswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m starttagopen\u001b[39m.\u001b[39mmatch(rawdata, i): \u001b[39m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_starttag(i)\n\u001b[0;32m    171\u001b[0m     \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32mc:\\Users\\kjy15\\anaconda3\\envs\\study\\lib\\html\\parser.py:340\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:endpos])\n\u001b[0;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m endpos\n\u001b[1;32m--> 340\u001b[0m \u001b[39mif\u001b[39;00m end\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m/>\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    341\u001b[0m     \u001b[39m# XHTML-style empty tag: <span attr=\"value\" />\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = datetime(2023, 2, 1)\n",
    "end_date = datetime(2023, 2, 2)\n",
    "\n",
    "# 크롤링할 URL 설정\n",
    "url = \"https://news.daum.net/breakingnews/digital\"\n",
    "\n",
    "# 크롤링할 기사 링크를 저장할 리스트 초기화\n",
    "links = set()\n",
    "\n",
    "# 주어진 기간 동안 각 페이지의 기사 링크 추출\n",
    "while start_date <= end_date:\n",
    "    # 현재 날짜에 해당하는 페이지 URL 생성\n",
    "    date_str = start_date.strftime(\"%Y%m%d\")\n",
    "    page_url = f\"{url}?regDate={date_str}\"\n",
    "    num_page = 1\n",
    "    last_page = False\n",
    "\n",
    "    # 각 페이지의 기사 링크 추출\n",
    "    while True:\n",
    "        page_url_num = f\"{page_url}&page={num_page}\"\n",
    "        res = requests.get(page_url_num)\n",
    "\n",
    "        # HTTP 요청이 성공했는지 확인\n",
    "        if res.status_code != 200:\n",
    "            print(f\"Failed to retrieve page: {page_url_num}\")\n",
    "            continue\n",
    "\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        # 각 기사의 링크를 추출하여 리스트에 추가\n",
    "        article_links = soup.select(\".list_news2 a.link_txt\")\n",
    "\n",
    "        if not article_links:\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            article_url = link[\"href\"]\n",
    "            article_res = requests.get(article_url)\n",
    "\n",
    "            # HTTP 요청이 성공했는지 확인\n",
    "            if article_res.status_code != 200:\n",
    "                print(f\"Failed to retrieve article: {article_url}\")\n",
    "                continue\n",
    "\n",
    "            article_soup = BeautifulSoup(article_res.content, \"html.parser\")\n",
    "            date = article_soup.select_one(\".info_view .txt_info\")\n",
    "\n",
    "            if date is None:\n",
    "                date = start_date.strftime(\"%Y.%m.%d.\")\n",
    "            else:\n",
    "                date = date.get_text().strip()\n",
    "\n",
    "            if (article_url, date) not in links:\n",
    "                links.add((article_url, date))\n",
    "\n",
    "        # 마지막 페이지 여부를 검사하여 last_page 변수에 할당\n",
    "        last_page = (len(article_links) < 10)\n",
    "        if last_page:\n",
    "            break\n",
    "\n",
    "        num_page += 1\n",
    "\n",
    "    # 다음 페이지로 넘어가기 위해 날짜 1일 증가\n",
    "    start_date += timedelta(days=1)\n",
    "\n",
    "\n",
    "# 추출된 링크를 이용하여 기사 내용 크롤링\n",
    "for link, date in links:\n",
    "    # 기사 URL 출력 (테스트용)\n",
    "    print(f\"\\n☞링크: {link}\")\n",
    "\n",
    "    # 기사 HTML 코드 가져오기\n",
    "    res = requests.get(link)\n",
    "\n",
    "    # HTTP 요청이 성공했는지 확인\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Failed to retrieve article: {link}\")\n",
    "        continue\n",
    "\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "    # 기사 제목 추출\n",
    "    title = soup.select_one(\".tit_view\").get_text()\n",
    "\n",
    "    # 기사 본문 추출\n",
    "    contents = soup.select(\".article_view p\")\n",
    "    content = \"\\n\".join([c.get_text().strip() for c in contents])\n",
    "\n",
    "    # 추출한 제목과 내용 출력\n",
    "    print(f\"☞기사제목: {title}\")\n",
    "    print(f\"☞작성일자: {date_str}\")\n",
    "    print(f\"☞기사내용: \\n{content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
