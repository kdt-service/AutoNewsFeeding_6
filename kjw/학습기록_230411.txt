오늘 공부한 것의 핵심은 Perceptron퍼셉트론(인공지능학습기)입니다.
(한국말로 하면 인공지능학습 같습니다.)
퍼셉트론이 인공 신경망 이라는 딥러닝의 한 종류로 저는 이해했습니다.
(선생님이 하시는 말씀을 듣고 혼자서 이 궁리 저 궁리 .... 연결해서 생각했을때 그럴 것 같습니다.)
인공 신경망이 입력과 출력이 있고 중간에 Hidden이 있는데 이 Hidden에 입력값과 가중치를 곱해서
더한 값을 넣어서 출력(output)이 나온다고 말씀하셨어요. 이렇게 나온 값을 y에 모자를 씌워서
y 햇(hat)이라고 하고 이 y햇이 '예측값'이 되는 것이고 y햇에서 모자를 벗긴 y값이 '실제값'입니다.
실제값과 예측값의 차이 즉 y - y햇 이 바로 비용 또는 손실이고 이 비용(또는 손실)을 계산하는 것을
비용함수(손실함수)라고 하고 이 손실함수를 최소로 줄여야 모델의 예측력이 높아지므로
딥러닝을 통해 손실함수를 최소로 만들어 줘야 된답니다.
​
[ 4/11일 학습내용 ]
1.  딥러닝
1) Linear Regression 선형회귀모델 복습 : 입력변수(x)의 선형결합으로 출력변수(y)를 표현. 경사하강법
2) Logistic Regression 로지스틱 회귀 : 0과 1사이의 범주형 y를 표현
3) 인공 신경망 파라미터
- 파라미터 : 가중치 (w, z)   : 학습 과정에서 모델이 조정하는 변수이다
- 하이퍼 파라미터:  모델의 학습 과정을 결정하는 변수이다. 이 변수들은 모델의 구조, 학습 알고리즘 등을 결정하며, 모델의 성능을 크게 좌우할 수 있다
    hidden layer(h1이라 함)
    hidden node(h2 라 함)
    activation function : 신경망의 출력값을 변형하여 다음 계층으로 전달하는 함수로,
                            비선형 함수를 사용해야 인공 신경망이 더 복잡한 패턴을 학습할 수 있다.
                            주로 사용되는 활성화 함수에는 sigmoid, ReLU(Rectified Linear Unit), tanh 등이 있습니다. 각 활성화 함수
4) 비용함수 : 인공 신경망에서 학습하는 과정에서 모델의 예측값과 실제값 간의 차이를 최소화하기 위해 사용되는 기법이다.
                이를 위해 학습 데이터의 손실(loss) 또는 비용(cost)을 계산하고, 이를 최소화하는 방향으로 모델의 파라미터를 업데이트합니다.
                손실은 예측값과 실제값 사이의 차이를 측정한 값으로,
                 이 값이 작을수록 모델의 예측력이 높아집니다.
                 비용은 전체 학습 데이터의 손실을 평균화한 값으로, 비용을 최소화하는 방향으로 모델을 학습시킵니다.
5) 역전파 (Backpropagation)
- 오차 역전파법(Backpropagation) 또는 오류 역전파 알고리즘은 다층 퍼셉트론 학습에 사용되는 통계적 기법
- 경사하강법(Gradient Descent): 손실함수의 기울기(=가중치의 편미분)에 학습 (learning rate)을 반영하여 가중치(파라미터)를 갱신하여 학습
​
※ 주의할 개념 : 노드(Node)와 피처(Feature) 노드는 인공 신경망이나 의사결정나무 등의 모델에서 사용되는 용어로,
                     모델 내부의 하나의 작은 계산 단위를 나타냅니다. 노드는 입력값과 가중치를 계산한 뒤,
                     활성화 함수를 거쳐 출력값을 계산합니다. 이러한 과정을 통해 모델이 입력값과 출력값 사이의 관계를 파악하며, 예측 또는 분류 등의 작업을 수행합니다.
                     
                      피처는 데이터의 속성(attribute)이나 변수(variable)로, 데이터를 분석하거나 모델링할 때 사용되는 입력 변수입니다.
                      예를 들어, 고객의 성별, 나이, 결혼 여부, 소득 수준 등이 모두 피처가 될 수 있습니다.
                      피처는 데이터의 차원(dimension)을 결정하며, 분석과 예측에 영향을 미칩니다.
                      노드는 모델의 내부 단위로, 피처는 모델의 입력 변수로 사용되며, 이를 통해 모델이 데이터를 분석하고 예측하는 과정을 수행합니다.​
※ 이해못한 개념
시그모이드 씌우는 이유
이진분류에서 사용하는 활성화함수가 시그모이드sigmoid함수이다.
-> 활성화함수중 하나가 시그모이드 함수 로서 이진분류에서 쓰이고, 다층분류에서는 softMax함수를 사용한다고 하는데... 왜 그런지? 안 물어봤네요...
    (자꾸 물어볼려니까.... 다른 분들한테 미안하고,,,, 너무 무식을 티 내는 것 같아서 chatGPT에 물어 본 내용이 아래와 같습니다.)
    why?  이진 분류에서는 출력값이 0 또는 1로 나와야 하는데, 시그모이드 함수의 출력 범위가 0과 1 사이이기 때문에 이진 분류에 적합합니다.
    시그모이드 함수는 입력값이 커지면 출력값이 1에 가까워지고, 입력값이 작아지면 출력값이 0에 가까워집니다. 따라서,
    이진 분류에서는 시그모이드 함수를 출력층의 활성화 함수로 사용해서 모델이 0 또는 1에 가까운 출력값을 출력할 수 있도록 합니다.
    반면, 다중 분류에서는 하나의 입력값에 대해 여러 개의 출력값을 예측해야 하기 때문에, 출력값들이 확률적인 의미를 가지도록 만들어야 합니다.
    이를 위해서는 출력값들을 0과 1 사이의  값으로 제한하면서, 출력값들의 합이 1이 되도록 정규화해야 합니다.
    이를 가능하게 하는 함수가 소프트맥스 함수입니다.
    소프트맥스 함수는 입력값에 대해 각 클래스에 속할 확률을 예측할 수 있도록 하며,
    다중 분류에서 출력층의 활성화 함수로 많이 사용됩니다.  따라서, 이진 분류에서는
    시그모이드Sigmoid 함수를 사용하고, 다중 분류에서는 소프트맥스 함수를 사용하는 것은 출력값의 형태와 확률적인 의미를 고려한 선택입니다.
​
​​
​